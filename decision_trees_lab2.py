# -*- coding: utf-8 -*-
"""Decision_Trees_Lab2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mxc3y0ohW1GR9ApTZpOjbEgwxaR04GH0

ΑΣΚΗΣΗ 2

**ΕΡΩΤΗΜΑ 1**

Φορτώνουμε τα δεδομένα σε DataFrame. Στη συνέχεια, περιγράφουμε (describe) και δημιουργούμε το ιστόγραμμα για κάθε χαρακτηριστικό, καθώς και ραβδόγραμμα για τη μεταβλητή Potability. Αναφέρουμε το πλήθος των ελλιπών τιμών για κάθε χαρακτηριστικό και τα ποσοστά πόσιμου και μη-πόσιμου νερού των παραδειγμάτων.

Φόρτωση Βιβλιοθηκών
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

"""Φόρτωση των δεδομένων μου σε DataFrame."""

df = pd.read_csv("/content/water_potability.csv")
pd.set_option('display.max_columns', None)
df

"""Χρήση της describe() μεθόδου."""

df.describe()

"""Δημιουργία Histogram και BarPlot για κάθε μεταβλητή."""

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='ph', kde=True)
plt.title('ph - Histogram')
plt.xlabel('Frequency')
plt.ylabel('ph values')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Hardness', kde=True)
plt.title('Hardness - Histogram')
plt.xlabel('Frequency')
plt.ylabel('Hardness - values')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Solids', kde=True)
plt.title('Solids - Histogram')
plt.xlabel('Frequency')
plt.ylabel('Solids - values')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Chloramines', kde=True)
plt.title('Chloramines - Histogram')
plt.xlabel('Frequency')
plt.ylabel('Chloramines - values')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Sulfate', kde=True)
plt.title('Sulfate - Histogram')
plt.xlabel('Frequency')
plt.ylabel('Sulfate - values')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Conductivity', kde=True)
plt.title('Conductivity - Histogram')
plt.xlabel('Frequency')
plt.ylabel('Conductivity - values')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Organic_carbon', kde=True)
plt.title('Organic_carbon - Histogram')
plt.xlabel('Frequency')
plt.ylabel('Organic_carbon - values')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Trihalomethanes', kde=True)
plt.title('Trihalomethanes - Histogram')
plt.xlabel('Frequency')
plt.ylabel('Trihalomethanes	- values')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Turbidity', kde=True)
plt.title('Turbidity - Histogram')
plt.xlabel('Frequency')
plt.ylabel('Turbidity - values')
plt.grid(True)
plt.show()

"""Για το Potability BarPlot αρχικά βρίσκω πόσες τιμές είναι 0 και πόσες 1."""

count_pot = df['Potability'].value_counts()
count_pot

"""Αφού βρω τις τιμές δημιουργώ το Bar Plot."""

count_pot.plot(kind='bar')
plt.ylabel('Frequency')
plt.title('Potability Results')

"""Εύρεση Πλήθους ελλιπών τιμών για κάθε χαρακτηριστικό με τη χρήση for loop.

"""

for column in df:
  missing_values = df[column].isnull().value_counts()
  print(missing_values)

"""Συμπεράσματα:
1. Missing Values: Υπάρχουν ελλειπείς τιμές ωστόσο, ξέρουμε οτι το δέντρο διαχειρίζεται αρκετά καλά τα datasets με missing values.
2. Κατανομή Χαρακτηριστικών: Με βάση τα Histograms παρατηρώ οτι υπάρχει ισορροπία στην κατανομή των χαρακτηριστικών.
3. Ποικιλία Χαρακτηριστικών: Απαραίτητη για μία πιο λεπτομερή και ολοκληρωμένη ανάλυση.
4. Παρουσία Outliers: Εξέταση ορθότητας ακραίων τιμών σε ορισμένα χαρακτηριστικά, ώστε να μην επηρεάσουν αρνητικα το αποτέλεσμα της ανάλυσης (εαν είναι σφάλματα).

**ΕΡΩΤΗΜΑ 2**

Σύμφωνα με τον Παγκόσμιο Οργανισμό Υγείας (ΠΟΥ), τα προτεινόμενα επίπεδα pH του νερού είναι 6.5 και 8.5. Υπολογίζουμε τα ποσοστά πόσιμου και μη-πόσιμου νερού των παραδειγμάτων για α) pH<6.5, β) 6.5≤pH≤8.5 και η) 8.5<pH. Σε τι βαθμό επαληθεύεται ο ισχυρισμός αυτός με βάση τον ΜΟ και την Τυπική Απόκλιση?

Ποσοστό πόσιμου νερού με ph < 6.5
"""

filtered_Df1 = df[(df['Potability'] == 1) & (df['ph'] < 6.5)]

percentage = (len(filtered_Df1) / len(df))*100

print("Percentage: Water potability = 1 & ph < 6.5 : ", percentage)

"""Ποσοστό μη-πόσιμου νερού με ph < 6.5"""

filtered_Df2 = df[(df['Potability'] == 0) & (df['ph'] < 6.5)]

percentage = (len(filtered_Df2) / len(df))*100

print("Percentage: Water potability = 0 & ph < 6.5 : ", percentage)

"""Ποσοστό πόσιμου νερού με 8.5 > ph > 6.5"""

filtered_Df3 = df[(df['Potability'] == 1) & (df['ph'] >= 6.5) & (df['ph'] <= 8.5)]

percentage = (len(filtered_Df3) / len(df))*100

print("Percentage: Water potability = 1 & 8.5 > ph > 6.5 : ", percentage)

"""Ποσοστό μη-πόσιμου νερού με 8.5 > ph > 6.5"""

filtered_Df4 = df[(df['Potability'] == 0) & (df['ph'] >= 6.5) & (df['ph'] <= 8.5)]

percentage = (len(filtered_Df4) / len(df))*100

print("Percentage: Water potability = 0 & 8.5 > ph > 6.5 : ", percentage)

"""Ποσοστό πόσιμου νερού με ph > 8.5"""

filtered_Df5 = df[(df['Potability'] == 1) & (df['ph'] > 8.5)]

percentage = (len(filtered_Df5) / len(df))*100

print("Percentage: Water potability = 1 & ph > 8.5 : ", percentage)

"""Ποσοστό μη-πόσιμου νερού με ph > 8.5"""

filtered_Df6 = df[(df['Potability'] == 0) & (df['ph'] > 8.5)]

percentage = (len(filtered_Df6) / len(df))*100

print("Percentage: Water potability = 0 & ph > 8.5 : ", percentage)

"""ΣΥΜΠΕΡΑΣΜΑΤΑ: Παρατηρούμε ότι ο μέσος όρος του ph = 7.080795, επομένως είναι εντώς των προτεινόμενων ορίων που δίνει ο ΠΟΥ. Η τυπική απόκλιση μας δείχνει τη διασπορά των τιμών του ph γύρω από το μέσο όρο και την πιθανή ύπαρξη τιμών ph εκτός των ορίων που δίνει ο ΠΟΥ.

**ΕΡΩΤΗΜΑ 3**

Σύμφωνα με τον ΠΟΥ, τα προτεινόμενα επίπεδα χλωραμίνης είναι έως 4 ppm. Δημιουργήστε διάγραμμα διασποράς (scatter plot) μεταξύ x: pH και y: Chloramine, στο οποίο θα χρωματίσετε τα πόσιμα παραδείγματα με μπλε και τα μη-πόσιμα με κόκκινο. Τι διαπιστώνουμε για τη διαχωρισιμότητα των παραδειγμάτων?
"""

sns.scatterplot(x=df['ph'],
                y=df['Chloramines'],
                hue=df['Potability'],
                data=df,
                palette={1:'red', 0:'blue'})

"""ΣΥΜΠΕΡΑΣΜΑΤΑ: Τα δεδομένα δεν είναι διαχωρίσιμα.

**ΕΡΩΤΗΜΑ 4**

Δημιουργούμε numpy arrays με κατάλληλα inputs (x) και targets (y), όπου target το potability. Χωρίζουμε τα δεδομένα σε train-test σε ποσοστό 70-30 αντίστοιχα με 0 seed.
"""

import sklearn
from sklearn.model_selection import train_test_split, GridSearchCV

"""Δημιουργώ numpy arrays με inputs & outputs."""

X = df.iloc[:, :-1].to_numpy() #inputs:όλες οι στήλες εκτός από την τελευταία (ανεξάρτητες μεταβλητές).
y = df.iloc[:, -1].to_numpy() #outputs:μόνο η τελευταία στήλη - potability - (εξαρτημένη μεταβλητή).

"""Χωρίζω τα δεδομένα σε train-test σε ποσοστό 70-30."""

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.3)

"""**ΕΡΩΤΗΜΑ 5**

Εκπαιδεύουμε ταξινομητή Δέντρο Απόφασης (Decision Tree) στο train set και ύστερα μετράμε την ακρίβεια του (accuracy) στα train, test σετ.
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

classifier = DecisionTreeClassifier(random_state=0)
classifier.fit(X_train, y_train)

classifier.get_params()

predictions = classifier.predict(X_test)

accuracy_score(y_test, predictions)

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, predictions)

"""**ΕΡΩΤΗΜΑ 6**

Επαναλαμβάνουμε το ερώτημα 5, δοκιμάζοντας τους παρακάτω συνδυασμούς: criterion (gini, entropy), max-depth (None, 3, 5), min-samples-split (2, 5, 10), min-samples-leaf (1, 2), max-features (None, sqrt), cost-complexity-pruning (0, 0.01). Δημιουργούμε πίνακα ακρίβειας σε train-test για κάθε συνδυασμό με pandas, όπου οι στήλες θα είναι οι τιμές κάθε χαρακτηριστικό, καθώς και train acc, test acc.

Δημιουργία λεξικού με όλες τις παραμέτρους και τις τιμές που θέλουμε να δοκιμάσουμε.
"""

path = classifier.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas

from math import sqrt
parameters = {
    'criterion': ['gini', 'entropy'],
    'max_depth':[None, 3, 5],
    'min_samples_split':[2, 5, 10],
    'min_samples_leaf':[1, 2],
    'max_features':[None, sqrt],
    'ccp_alpha':[0, 0.01]
    }

GS = GridSearchCV(estimator = classifier,
                  param_grid = parameters,
                  scoring = ["accuracy"],
                  refit = "accuracy",
                  )

GS.fit(X_train, y_train)

"""**ΕΡΩΤΗΜΑ 7**

Επιλέγουμε τον συνδυασμό με τη μεγαλύτερη ακρίβεια (acc) στο test όταν max-depth = 3. Στη συνέχεια, εκπαιδεύουμε πάλι το δέντρο και απεικονίζουμε τη δομή του https://scikit-learn.org/1.5/modules/generated/sklearn.tree.plot_tree.html.

Εκπαιδεύω το δέντρο για depth = 3.
"""

parameters2 = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [3],
    'min_samples_split':[2, 5, 10],
    'min_samples_leaf':[1, 2],
    'max_features':[None, sqrt],
    'ccp_alpha':[0, 0.01]
    }

GS2 = GridSearchCV(estimator = classifier,
                  param_grid = parameters2,
                  scoring = ["accuracy"],
                  refit = "accuracy",
                  )

GS2.fit(X_train, y_train)

classifier2 = DecisionTreeClassifier(ccp_alpha=0, max_depth=3, random_state=0)
classifier2.fit(X_train, y_train)

"""Απεικόνιση της δομής του δέντρου"""

from sklearn import tree
tree.plot_tree(classifier2)

"""**ΕΡΩΤΗΜΑ 8**

Επιλέγουμε το συνδυασμό με τη μεγαλύτερη ακρίβεια στο test, επανεκπαιδεύουμε το δέντρο και δημιουργούμε ραβδόγραμμα με τη σημαντικότητα κάθε χαρακτηριστικού (feature importance) σύμφωνα με το δέντρο αυτό. Αν η συσκευή μπορεί να υποστηρίζει πολλές χημικές αναλύσεις, ποιες είναι οι 5 πιο σημαντικές?

"""

classifier = DecisionTreeClassifier(ccp_alpha=0, max_depth=5, min_samples_leaf=2,

                       min_samples_split=5, random_state=0)
classifier.fit(X_train, y_train)

print("Coefficients: \n", classifier.feature_importances_)
features = np.array(['ph', 'Hardness',
                     'Solids', 'Chloramines',
                     'Sulfate', 'Conductivity',
                     'Organic_carbon', 'Trihalomethanes', 'Turbidity'])

coef_df = pd.DataFrame({'Features': features, 'coefficients': classifier.feature_importances_})
print(coef_df)

# Sort feature importances in descending order
indices = np.argsort(classifier.feature_importances_)[::-1]

# Create plot
plt.figure(figsize=(10, 6))
plt.title("Feature Importances")
plt.bar(range(X.shape[1]), classifier.feature_importances_[indices])
plt.xticks(range(X.shape[1]), features, rotation=90)
plt.xlabel("Features")
plt.ylabel("Importance")
plt.show()

"""ΣΥΜΠΕΡΑΣΜΑΤΑ: Με βάση το παραπάνω Bar Plot τα χαρακτηριστικά με τα μεγαλύτερα coeffients είναι αυτα που επηρεάζουν τα αποτελέσματα της ανάλυσης του νερού. Αυτά τα χαρακτηριστικά είναι τα ph, Hardness, Solid, Chloramines και Sulfate.

**ΕΡΩΤΗΜΑ 9**

Εξηγούμε τους λόγους για τους οποίους ένα Τυχαίο Δάσος (Random Forest) ενδεχομένως να πετύχαινε καλύτερη ακρίβεια από το Δέντρο Απόφασης.

Το τυχαίο δάσος (Random Forest) απαρτίζεται από ένα σύνολο δέντρων απόφασης (Decision Tree), άρα αυτομάτως έχει μεγαλύτερη πιθανότητα να πετύχει καλύτερη ακρίβεια σε σχέση με ένα δέντρο απόφασης.

**ΕΡΩΤΗΜΑ 10**

 Επαναλαμβάνουμε το ερώτημα 6 χρησιμοποιώντας Random Forest
 https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html . Ορίζουμε το 0 ως seed. Επιπλέον, στους συνδυασμούς να προστίθεται και το πλήθος των δέντρων (n_estimators) για 50, 100 και 200 δέντρα.
"""

from sklearn.ensemble import RandomForestClassifier

classifier3 = RandomForestClassifier(random_state=0)
classifier3.fit(X_train, y_train)

parameters3 = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 3, 5],
    'min_samples_split':[2, 5, 10],
    'min_samples_leaf':[1, 2],
    'max_features':[None, sqrt],
    'n_estimators':[50, 100, 200]
    }

GS3 = GridSearchCV(estimator = classifier3,
                  param_grid = parameters3,
                  scoring = ["accuracy"],
                  refit = "accuracy",
                  )

GS3.fit(X_train, y_train)

"""**ΕΡΩΤΗΜΑ 11**

Τι είναι πιο σημαντικό για το μοντέλο, να προβλέπει σωστά το πόσιμο νερό, αλλά χάνοντας ακρίβεια από το μη-πόσιμο νερό ή να προβλέπει σωστά το μη-πόσιμο νερό, χάνοντας ακρίβεια από το πόσιμο?

ΑΠΑΝΤΗΣΗ:

Το πιο σημαντικό είναι το μοντέλο μου να προβλέπει σωστά το μη-πόσιμο νερό, χάνοντας ακρίβεια από το πόσιμο. Η αποτυχία πρόβλεψης πόσιμου νερού πολύ πιθανό να φέρει σε κίνδυνο την υγεία των ανθρώπων.

**ΕΡΩΤΗΜΑ 12**

Επιλέγουμε το συνδυασμό με τη μεγαλύτερη ακρίβεια στο test, επανεκπαιδεύστε το δάσος και δημιουργήστε ραβδόγραμμα με τη σημαντικότητα κάθε χαρακτηριστικού (feature importance) σύμφωνα με το δέντρο αυτό. Αν η συσκευή μπορεί να υποστηρίζει πολλές χημικές αναλύσεις, ποιες είναι οι 5 πιο σημαντικές που θα έπρεπε να εξάγει από το νερό σύμφωνα με το δάσος? Διαφέρουν σε σχέση με το καλύτερο δέντρο (Ερώτηση 8)? Αν ναι, γιατί διαφέρουν.
"""

classifier4 = RandomForestClassifier(max_features=None, n_estimators=50, random_state=0)
classifier4.fit(X_train, y_train)

print("Coefficients: \n", classifier4.feature_importances_)
features = np.array(['ph', 'Hardness',
                     'Solids', 'Chloramines',
                     'Sulfate', 'Conductivity',
                     'Organic_carbon', 'Trihalomethanes', 'Turbidity'])

coef_df4 = pd.DataFrame({'Features': features, 'coefficients': classifier4.feature_importances_})
print(coef_df4)

# Sort feature importances in descending order
indices = np.argsort(classifier3.feature_importances_)[::-1]

# Create plot
plt.figure(figsize=(10, 6))
plt.title("Feature Importances")
plt.bar(range(X.shape[1]), classifier4.feature_importances_[indices])
plt.xticks(range(X.shape[1]), features, rotation=90)
plt.xlabel("Features")
plt.ylabel("Importance")
plt.show()

"""ΣΥΜΠΕΡΑΣΜΑΤΑ: Σύμφωνα με το Random Forest και με βάση το παραπάνω Bar Plot τα χαρακτηριστικά με τα μεγαλύτερα coeffients είναι αυτα που επηρεάζουν τα αποτελέσματα της ανάλυσης του νερού. Αυτά τα χαρακτηριστικά είναι τα ph, Hardness, Solid, Chloramines και Sulfate.

**ΕΡΩΤΗΜΑ 13**

Τι είναι νομικά ασφαλέστερο για την εταιρία, η χρήση του καλύτερου δέντρου ή του καλύτερου τυχαίου δάσους?

Καλύτερη επιλογή αποτελεί το απλό δέντρο διότι γιατί ειναι πιο εύκολα επεξηγήσιμο σε περίπτωση που κάτι πάει στραβά όσο αφορά στην πρόβλεψη μας.

-----------------------------------------------------------------------
"""