# -*- coding: utf-8 -*-
"""Decision_Trees_Lab2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mxc3y0ohW1GR9ApTZpOjbEgwxaR04GH0

ΑΣΚΗΣΗ 2

**ΕΡΩΤΗΜΑ 1**

Φόρτωση Βιβλιοθηκών
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

"""Φόρτωση των δεδομένων μου σε DataFrame."""

df = pd.read_csv("/content/water_potability.csv")
pd.set_option('display.max_columns', None)
df

"""Χρήση της describe() μεθόδου."""

df.describe()

"""Δημιουργία Histogram και BarPlot για κάθε μεταβλητή."""

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='ph', kde=True)
plt.title('ph - Histogram')
plt.xlabel('Frequency')
plt.ylabel('ph values')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Hardness', kde=True)
plt.title('Hardness - Histogram')
plt.xlabel('Frequency')
plt.ylabel('Hardness - values')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Solids', kde=True)
plt.title('Solids - Histogram')
plt.xlabel('Frequency')
plt.ylabel('Solids - values')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Chloramines', kde=True)
plt.title('Chloramines - Histogram')
plt.xlabel('Frequency')
plt.ylabel('Chloramines - values')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Sulfate', kde=True)
plt.title('Sulfate - Histogram')
plt.xlabel('Frequency')
plt.ylabel('Sulfate - values')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Conductivity', kde=True)
plt.title('Conductivity - Histogram')
plt.xlabel('Frequency')
plt.ylabel('Conductivity - values')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Organic_carbon', kde=True)
plt.title('Organic_carbon - Histogram')
plt.xlabel('Frequency')
plt.ylabel('Organic_carbon - values')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Trihalomethanes', kde=True)
plt.title('Trihalomethanes - Histogram')
plt.xlabel('Frequency')
plt.ylabel('Trihalomethanes	- values')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Turbidity', kde=True)
plt.title('Turbidity - Histogram')
plt.xlabel('Frequency')
plt.ylabel('Turbidity - values')
plt.grid(True)
plt.show()

"""Για το Potability BarPlot αρχικά βρίσκω πόσες τιμές είναι 0 και πόσες 1."""

count_pot = df['Potability'].value_counts()
count_pot

"""Αφού βρω τις τιμές δημιουργώ το Bar Plot."""

count_pot.plot(kind='bar')
plt.ylabel('Frequency')
plt.title('Potability Results')

"""Εύρεση Πλήθους ελλιπών τιμών για κάθε χαρακτηριστικό με τη χρήση for loop.

"""

for column in df:
  missing_values = df[column].isnull().value_counts()
  print(missing_values)

"""Συμπεράσματα:
1. Missing Values: Υπάρχουν ελλειπείς τιμές ωστόσο, ξέρουμε οτι το δέντρο διαχειρίζεται αρκετά καλά τα datasets με missing values.
2. Κατανομή Χαρακτηριστικών: Με βάση τα Histograms παρατηρώ οτι υπάρχει ισορροπία στην κατανομή των χαρακτηριστικών.
3. Ποικιλία Χαρακτηριστικών: Απαραίτητη για μία πιο λεπτομερή και ολοκληρωμένη ανάλυση.
4. Παρουσία Outliers: Εξέταση ορθότητας ακραίων τιμών σε ορισμένα χαρακτηριστικά, ώστε να μην επηρεάσουν αρνητικα το αποτέλεσμα της ανάλυσης (εαν είναι σφάλματα).

**ΕΡΩΤΗΜΑ 2**

Ποσοστό πόσιμου νερού με ph < 6.5
"""

filtered_Df1 = df[(df['Potability'] == 1) & (df['ph'] < 6.5)]

percentage = (len(filtered_Df1) / len(df))*100

print("Percentage: Water potability = 1 & ph < 6.5 : ", percentage)

"""Ποσοστό μη-πόσιμου νερού με ph < 6.5"""

filtered_Df2 = df[(df['Potability'] == 0) & (df['ph'] < 6.5)]

percentage = (len(filtered_Df2) / len(df))*100

print("Percentage: Water potability = 0 & ph < 6.5 : ", percentage)

"""Ποσοστό πόσιμου νερού με 8.5 > ph > 6.5"""

filtered_Df3 = df[(df['Potability'] == 1) & (df['ph'] >= 6.5) & (df['ph'] <= 8.5)]

percentage = (len(filtered_Df3) / len(df))*100

print("Percentage: Water potability = 1 & 8.5 > ph > 6.5 : ", percentage)

"""Ποσοστό μη-πόσιμου νερού με 8.5 > ph > 6.5"""

filtered_Df4 = df[(df['Potability'] == 0) & (df['ph'] >= 6.5) & (df['ph'] <= 8.5)]

percentage = (len(filtered_Df4) / len(df))*100

print("Percentage: Water potability = 0 & 8.5 > ph > 6.5 : ", percentage)

"""Ποσοστό πόσιμου νερού με ph > 8.5"""

filtered_Df5 = df[(df['Potability'] == 1) & (df['ph'] > 8.5)]

percentage = (len(filtered_Df5) / len(df))*100

print("Percentage: Water potability = 1 & ph > 8.5 : ", percentage)

"""Ποσοστό μη-πόσιμου νερού με ph > 8.5"""

filtered_Df6 = df[(df['Potability'] == 0) & (df['ph'] > 8.5)]

percentage = (len(filtered_Df6) / len(df))*100

print("Percentage: Water potability = 0 & ph > 8.5 : ", percentage)

"""ΣΥΜΠΕΡΑΣΜΑΤΑ: Παρατηρούμε ότι ο μέσος όρος του ph = 7.080795, επομένως είναι εντώς των προτεινόμενων ορίων που δίνει ο ΠΟΥ. Η τυπική απόκλιση μας δείχνει τη διασπορά των τιμών του ph γύρω από το μέσο όρο και την πιθανή ύπαρξη τιμών ph εκτός των ορίων που δίνει ο ΠΟΥ.

ΕΡΩΤΗΜΑ 3
"""

sns.scatterplot(x=df['ph'],
                y=df['Chloramines'],
                hue=df['Potability'],
                data=df,
                palette={1:'red', 0:'blue'})

"""ΣΥΜΠΕΡΑΣΜΑΤΑ: Τα δεδομένα μου δεν είναι διαχωρίσιμα.

ΕΡΩΤΗΜΑ 4
"""

import sklearn
from sklearn.model_selection import train_test_split, GridSearchCV

"""Δημιουργώ numpy arrays με inputs & outputs."""

X = df.iloc[:, :-1].to_numpy() #inputs:όλες οι στήλες εκτός από την τελευταία (ανεξάρτητες μεταβλητές).
y = df.iloc[:, -1].to_numpy() #outputs:μόνο η τελευταία στήλη - potability - (εξαρτημένη μεταβλητή).

"""Χωρίζω τα δεδομένα σε train-test σε ποσοστό 70-30."""

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.3)

"""ΕΡΩΤΗΜΑ 5

Εκπαίδευση ταξινομητή Δέντρο Απόφασης & μέτρηση ακρίβειας στα train - test set.
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

classifier = DecisionTreeClassifier(random_state=0)
classifier.fit(X_train, y_train)

classifier.get_params()

predictions = classifier.predict(X_test)

accuracy_score(y_test, predictions)

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, predictions)

"""ΕΡΩΤΗΜΑ 6

Δημιουργία λεξικού με όλες τις παραμέτρους και τις τιμές που θέλω να δοκιμάσω.
"""

path = classifier.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas

from math import sqrt
parameters = {
    'criterion': ['gini', 'entropy'],
    'max_depth':[None, 3, 5],
    'min_samples_split':[2, 5, 10],
    'min_samples_leaf':[1, 2],
    'max_features':[None, sqrt],
    'ccp_alpha':[0, 0.01]
    }

GS = GridSearchCV(estimator = classifier,
                  param_grid = parameters,
                  scoring = ["accuracy"],
                  refit = "accuracy",
                  )

GS.fit(X_train, y_train)

"""ΕΡΩΤΗΜΑ 7

Εκπαιδεύω το δέντρο για depth = 3.
"""

parameters2 = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [3],
    'min_samples_split':[2, 5, 10],
    'min_samples_leaf':[1, 2],
    'max_features':[None, sqrt],
    'ccp_alpha':[0, 0.01]
    }

GS2 = GridSearchCV(estimator = classifier,
                  param_grid = parameters2,
                  scoring = ["accuracy"],
                  refit = "accuracy",
                  )

GS2.fit(X_train, y_train)

classifier2 = DecisionTreeClassifier(ccp_alpha=0, max_depth=3, random_state=0)
classifier2.fit(X_train, y_train)

"""Απεικόνιση της δομής του δέντρου"""

from sklearn import tree
tree.plot_tree(classifier2)

"""Συμπέρασματα: ΔΕΝ ΚΑΤΑΛΑΒΑΙΝΩ ΤΙ ΠΡΕΠΕΙ ΝΑ ΠΕΡΙΓΡΑΨΩ ΑΚΡΙΒΩΣ ΑΠΟ ΤΟ ΔΕΝΤΡΟ.

ΕΡΩΤΗΜΑ 8
"""

classifier = DecisionTreeClassifier(ccp_alpha=0, max_depth=5, min_samples_leaf=2,

                       min_samples_split=5, random_state=0)
classifier.fit(X_train, y_train)

print("Coefficients: \n", classifier.feature_importances_)
features = np.array(['ph', 'Hardness',
                     'Solids', 'Chloramines',
                     'Sulfate', 'Conductivity',
                     'Organic_carbon', 'Trihalomethanes', 'Turbidity'])

coef_df = pd.DataFrame({'Features': features, 'coefficients': classifier.feature_importances_})
print(coef_df)

# Sort feature importances in descending order
indices = np.argsort(classifier.feature_importances_)[::-1]

# Create plot
plt.figure(figsize=(10, 6))
plt.title("Feature Importances")
plt.bar(range(X.shape[1]), classifier.feature_importances_[indices])
plt.xticks(range(X.shape[1]), features, rotation=90)
plt.xlabel("Features")
plt.ylabel("Importance")
plt.show()

"""ΣΥΜΠΕΡΑΣΜΑΤΑ: Με βάση το παραπάνω Bar Plot τα χαρακτηριστικά με τα μεγαλύτερα coeffients είναι αυτα που επηρεάζουν τα αποτελέσματα της ανάλυσης του νερού. Αυτά τα χαρακτηριστικά είναι τα ph, Hardness, Solid, Chloramines και Sulfate.

ΕΡΩΤΗΜΑ 9

Το τυχαίο δάσος (Random Forest) απαρτίζεται από ένα σύνολο δέντρων απόφασης (Decision Tree), άρα αυτομάτως έχει μεγαλύτερη πιθανότητα να πετύχει καλύτερη ακρίβεια σε σχέση με ένα δέντρο απόφασης.

ΕΡΩΤΗΜΑ 10

Επαναλαμβάνω το ερώτημα 6 χρησιμοποιώντας Random Forest.
"""

from sklearn.ensemble import RandomForestClassifier

classifier3 = RandomForestClassifier(random_state=0)
classifier3.fit(X_train, y_train)

parameters3 = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 3, 5],
    'min_samples_split':[2, 5, 10],
    'min_samples_leaf':[1, 2],
    'max_features':[None, sqrt],
    'n_estimators':[50, 100, 200]
    }

GS3 = GridSearchCV(estimator = classifier3,
                  param_grid = parameters3,
                  scoring = ["accuracy"],
                  refit = "accuracy",
                  )

GS3.fit(X_train, y_train)

"""ΕΡΩΤΗΜΑ 11

Το πιο σημαντικό είναι το μοντέλο μου να προβλέπει σωστά το μη-πόσιμο νερό, χάνοντας ακρίβεια από το πόσιμο. Η αποτυχία πρόβλεψης πόσιμου νερού πολύ πιθανό να φέρει σε κίνδυνο την υγεία των ανθρώπων.

ΕΡΩΤΗΜΑ 12
"""

classifier4 = RandomForestClassifier(max_features=None, n_estimators=50, random_state=0)
classifier4.fit(X_train, y_train)

print("Coefficients: \n", classifier4.feature_importances_)
features = np.array(['ph', 'Hardness',
                     'Solids', 'Chloramines',
                     'Sulfate', 'Conductivity',
                     'Organic_carbon', 'Trihalomethanes', 'Turbidity'])

coef_df4 = pd.DataFrame({'Features': features, 'coefficients': classifier4.feature_importances_})
print(coef_df4)

# Sort feature importances in descending order
indices = np.argsort(classifier3.feature_importances_)[::-1]

# Create plot
plt.figure(figsize=(10, 6))
plt.title("Feature Importances")
plt.bar(range(X.shape[1]), classifier4.feature_importances_[indices])
plt.xticks(range(X.shape[1]), features, rotation=90)
plt.xlabel("Features")
plt.ylabel("Importance")
plt.show()

"""ΣΥΜΠΕΡΑΣΜΑΤΑ: Σύμφωνα με το Random Forest και με βάση το παραπάνω Bar Plot τα χαρακτηριστικά με τα μεγαλύτερα coeffients είναι αυτα που επηρεάζουν τα αποτελέσματα της ανάλυσης του νερού. Αυτά τα χαρακτηριστικά είναι τα ph, Hardness, Solid, Chloramines και Sulfate.

ΕΡΩΤΗΜΑ 13

Καλύτερη επιλογή αποτελεί το απλό δέντρο διότι γιατί ειναι πιο εύκολα επεξηγήσιμο σε περίπτωση που κάτι πάει στραβά όσο αφορά στην πρόβλεψη μας.

-----------------------------------------------------------------------

ΑΣΚΗΣΗ 3

ΕΡΩΤΗΜΑ 1
"""

!pip install --upgrade plotly

import yfinance as yf

ticker_symbol = "DX-Y.NYB"
ticker = yf.Ticker(ticker_symbol)
df = ticker.history(period="5y").reset_index(drop=False)
df

new_df = df.drop(columns=['Volume', 'Dividends', 'Stock Splits'])
new_df

import plotly.express as px
import plotly.graph_objects as go

fig = go.Figure()
fig.add_trace(go.Scatter(x=new_df['Date'], y=df['Open'], mode='lines', name='Open'))
fig.add_trace(go.Scatter(x=new_df['Date'], y=df['High'], mode='lines', name='High'))
fig.add_trace(go.Scatter(x=new_df['Date'], y=df['Low'], mode='lines', name='Low'))
fig.add_trace(go.Scatter(x=new_df['Date'], y=df['Close'], mode='lines', name='Close'))

"""ΕΡΩΤΗΜΑ 2

Οι χρονοσειρές είναι μη-στατικές εφόσον μεταβάλλονται από μέρα σε μέρα.
Περιοδικά Μοτίβα - Εποχικότητα σε συγκεκριμένα χρονικά διαστήματα.

ΕΡΩΤΗΜΑ 3

Για τον Οκτώβριο του 2024 παρατηρώ μεν μία πτωτική τάση λόγω της σύγκρουσης Ιράν-Ισραήλ και της εισβολής της Ρωσίας στην Ουκρανία.

ΕΡΩΤΗΜΑ 4

Τα open, high, low και close είναι features, των οποίων οι τιμές τους έχουν μία χρονική διαδοχή, δηλαδή για την πρόβλεψη των αυριανιών τιμών, μπορούν να μας βοηθήσουν οι χθεσινές.

Η συνάρτηση περιγράφει τη σχέση που έχει το close της επόμενης ημέρας σε σχέση με τα χαρακτηριστικά open, high, low και close της χθεσινής.

ΕΡΩΤΗΜΑ 5

Εφόσον αναφερόμαστε σε χρονοσειρές ο τυχαίος διαχωρισμός δεν μπορεί να κριθεί κατάλληλος. Τα δεδομένα μας είναι εξαρτημένα χρονικά και ο τυχαίος διαχωρισμός πολύ πιθανόν να μη βοηθούσε σε σωστή πρόβλεψη.

ΕΡΩΤΗΜΑ 6
"""

#df['Date'] = pd.to_datetime(df['Date'])
type(new_df['Date'])

split_date = pd.to_datetime('2024-01-01').tz_localize('America/New_York')

df_train = new_df.loc[new_df['Date'] <= split_date]
df_test = df.loc[new_df['Date'] > split_date]

df_train2 = df_train.drop(columns=['Date'])
df_test2 = df_test.drop(columns=['Date'])

"""ΕΡΩΤΗΜΑ 7"""

def f(data_set, N):

  inputs = []
  targets = []

  for i in range(len(data_set) - N):
    x_timeFrame = data_set.iloc[i:i + N][['Open', 'High', 'Low', 'Close']]
    inputs.append(x_timeFrame)

    y_timeFrame = data_set.iloc[i + N][['Close']]
    targets.append(y_timeFrame)

  inputs = np.array(inputs)
  targets = np.array(targets)

  return inputs, targets

f(df_train2, 3)

"""ΕΡΩΤΗΜΑ 8

Προφανώς εάν θέλουμε να κάνουμε πρόβλεψη για την επόμενη ημέρα το μέγεθος του timeframe δεν θα είναι πολύ μεγάλο, άρα το Ν θα είναι μικρό. Στην περίπτωση που θέλουμε να κάνουμε πρόβλεψη για ένα μήνα μετά το Ν θα είναι μεγαλύτερο το οποίο θα μας βοηθήσει στην αναγνώρισή επαναλαμβανόμενων μοτίβων (εποχική διακύμανση), εντός οποιαδήποτε σταθερής περιόδου.

ΕΡΩΤΗΜΑ 9

Διαχωρισμός των δεδομένων train & test
"""

X_train, y_train = f(df_train2, 5)
X_test, y_test = f(df_test2, 5)

print(X_train.shape)

reshaped_Xtrain = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])
reshaped_Xtest = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])

print(reshaped_Xtest.shape)

"""ΕΡΩΤΗΜΑ 10"""

from sklearn.linear_model import LinearRegression

model_1 = LinearRegression()
model_1.fit(reshaped_Xtrain, y_train)
y_pred = model_1.predict(reshaped_Xtest)

from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_test, y_pred)

"""--------------------------------------------------------------------------------"""

from sklearn.ensemble import RandomForestRegressor, VotingRegressor

rf=RandomForestRegressor(n_estimators=100, random_state=0)

model_2 = VotingRegressor(estimators=[('rf', rf)])
model_2.fit(reshaped_Xtrain, y_train)

y_pred = model_2.predict(reshaped_Xtest)
mean_absolute_error(y_test, y_pred)

"""-----------------------------------------------------------------------------"""

from sklearn.ensemble import BaggingRegressor

Lr = LinearRegression()
model_3 = BaggingRegressor(estimator=Lr,
                        n_estimators=10, random_state=0).fit(reshaped_Xtrain, y_train)

y_pred = model_3.predict(reshaped_Xtest)
mean_absolute_error(y_test, y_pred)

"""-----------------------------------------------------------------------------"""

import xgboost as xgb

model_4 = xgb.XGBRegressor()
model_4.fit(reshaped_Xtrain, y_train)

y_pred = model_4.predict(reshaped_Xtest)
mean_absolute_error(y_test, y_pred)

"""--------------------------------------------------------------------------------"""

from sklearn.ensemble import StackingRegressor

Lr = LinearRegression()
model_5 = StackingRegressor(estimators = [('lr', Lr)]).fit(reshaped_Xtrain, y_train)
model_5.fit(reshaped_Xtrain, y_train)

y_pred = model_5.predict(reshaped_Xtest)
mean_absolute_error(y_test, y_pred)

"""--------------------------------------------------------------------------------"""

models = ['Linear Regression', 'Random Forest', 'Bagging Regressor', 'XGBoost', 'Stacking Regressor']

for i in models:
  if i == 'Linear Regression':
    model = LinearRegression()
  elif i == 'Random Forest':
    rf=RandomForestRegressor(n_estimators=100, random_state=0)
    model = VotingRegressor(estimators=[('rf', rf)])
  elif i == 'Bagging Regressor':
    Lr = LinearRegression()
    model = BaggingRegressor(estimator=Lr, n_estimators=10, random_state=0)
  elif i == 'XGBoost':
    model = xgb.XGBRegressor()
  elif i == 'Stacking Regressor':
    Lr = LinearRegression()
    model == StackingRegressor(estimators = [('lr', Lr)]).fit(reshaped_Xtrain, y_train)


    model.fit(reshaped_Xtrain, y_train)
    y_pred = model.predict(reshaped_Xtest)
    mean_absolute_error(y_test, y_pred)

